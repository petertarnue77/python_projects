{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12317faa-783d-4c35-83ca-96bdc932a3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love is coming\n"
     ]
    }
   ],
   "source": [
    "print(\"Love is coming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59d815ef-d4a3-4b41-ad7f-2cc259bc60e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love\n"
     ]
    }
   ],
   "source": [
    "print(\"Love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "317adf6c-9b59-4c23-b8cb-e3eb75886f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Steps in Applied Machine Learning:\n",
    "# 1. Load Library\n",
    "# 2. Load Dataset to which Machine Learning Algorithm to be applied\n",
    "#    Either a) load from a CSV file or b) load from a Database   \n",
    "# 3. Summarisation of Data to understand dataset (Descriptive Statistics)\n",
    "# 4. Visualisation of Data to understand dataset (Plots, Graphs etc.)\n",
    "# 5. Data pre-processing & Data transformation (split into train-test datasets)\n",
    "# 6. Application of a Machine Learning Algorithm to training dataset \n",
    "#   a) setup a ML algorithm and parameter settings\n",
    "#   b) cross validation setup with training dataset\n",
    "#   c) training & fitting Algorithm with training Dataset\n",
    "#   d) evaluation of trained Algorithm (or Model) and result\n",
    "#   e) save the trained model for future prediction\n",
    "# 7. Load the saved model and apply it to new dataset for prediction             \n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d7a820-4007-4f54-aa72-1ec54f39bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Load necessary libraries\n",
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a11d9794-5eb9-4439-99d1-abdcf61aa6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load DataSet from CSV file\n",
    "def loadFrCSVFile(filename):\n",
    "    print(filename)\n",
    "    col_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "    dataset = pd.read_csv(filename, names=col_names)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd26f531-e80c-4d5f-8e99-09963ac68f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import DataSet to a MySQL Database\n",
    "def import2MySQL(dataset):\n",
    "    engine_str = (\n",
    "            'mysql+pymysql://{user}:{password}@{server}/{database}'.format(\n",
    "            user      =  'root',\n",
    "            password  =  'root888',\n",
    "            server    =  'localhost',\n",
    "            database  =  'DataScienceRecipes'))\n",
    "    \n",
    "    engine = sa.create_engine(engine_str)\n",
    "    conn = engine.connect()\n",
    "    \n",
    "    #check whether connection is Successful or not\n",
    "    #if (conn): print(\"MySQL Connection is Successful ... ... ...\")    \n",
    "    #else:      print(\"MySQL Connection is not Successful ... ... ...\")\n",
    "    \n",
    "    dataset.to_sql(name='irisdata', con=engine, schema='datasciencerecipes', \n",
    "                   if_exists = 'replace', chunksize = 1000, index=False)\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d93e108c-0753-437f-b5fa-71fb2fa1ab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load DataSet from MySQL Database to Pandas a DataFrame\n",
    "def loadDataSetFrMySQLTable():\n",
    "    engine_str = (\n",
    "            'mysql+pymysql://{user}:{password}@{server}/{database}'.format(\n",
    "            user      =  'root',\n",
    "            password  =  'root888',\n",
    "            server    =  'localhost',\n",
    "            database  =  'datasciencerecipes'))\n",
    "    \n",
    "    engine = sa.create_engine(engine_str)\n",
    "    conn = engine.connect()\n",
    "    \n",
    "    #check whether connection is Successful or not\n",
    "    if (conn): print(\"MySQL Connection is Successful ... ... ...\")    \n",
    "    else:      print(\"MySQL Connection is not Successful ... ... ...\")\n",
    "    \n",
    "    # MySQL Query with few generated Attributes/Features\n",
    "    query = '''\n",
    "    SELECT  sepal_length, \n",
    "            sepal_width, \n",
    "            petal_length, \n",
    "            petal_width, \n",
    "            round(sepal_length/sepal_width,2) as ratio1, \n",
    "            round(sepal_width/petal_length,2) as ratio2,\n",
    "            round(petal_length/petal_width,2) as ratio3,\n",
    "            round(petal_width/sepal_length,2) as ratio4,\n",
    "            round(sepal_width/sepal_length,2) as ratio5, \n",
    "            round(petal_length/sepal_width,2) as ratio6,\n",
    "            round(petal_width/petal_length,2) as ratio7,\n",
    "            round(sepal_length/petal_width,2) as ratio8,\n",
    "            class \n",
    "    FROM irisdata;\n",
    "    '''\n",
    "    query_result = conn.execute(query)\n",
    "    dataset =  pd.DataFrame(query_result.fetchall(), \n",
    "                            columns =  query_result.keys())\n",
    "    print('DataFrame Size',dataset.shape);\n",
    "    print('ROW',dataset.shape[0]);print('COLUMN',dataset.shape[1]);\n",
    "    conn.close()\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bf26236-f103-435c-8219-cd8c34769f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Summarisation (Descriptive Statistics)\n",
    "def summariseDataset(dataset):\n",
    "    cols1 = ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "    cols2 = ['ratio1','ratio2','ratio3','ratio4']\n",
    "    cols3 = ['ratio5','ratio6','ratio7','ratio8']    \n",
    "    # shape\n",
    "    print(dataset[cols1].shape)\n",
    "    print(dataset[cols2].shape)\n",
    "    print(dataset[cols3].shape)    \n",
    "    # head\n",
    "    print(dataset[cols1].head(5))\n",
    "    print(dataset[cols2].head(5))\n",
    "    print(dataset[cols3].head(5))    \n",
    "    # descriptions\n",
    "    print(dataset[cols1].describe())\n",
    "    print(dataset[cols2].describe())    \n",
    "    print(dataset[cols3].describe())\n",
    "    # class distribution\n",
    "    print(dataset.groupby('class').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6d77a9b-d174-492a-a32f-a5a85b6ce095",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Visualisation to understand Data\n",
    "def visualiseDataset(dataset):\n",
    "    cols1 = ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "    cols2 = ['ratio1','ratio2','ratio3','ratio4']\n",
    "    cols3 = ['ratio5','ratio6','ratio7','ratio8'] \n",
    "    \n",
    "    # box and whisker plots\n",
    "    dataset[cols1].plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
    "    pyplot.show()\n",
    "    dataset[cols2].plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
    "    pyplot.show()\n",
    "    dataset[cols3].plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\n",
    "    pyplot.show()    \n",
    "    # histograms\n",
    "    dataset[cols1].hist()\n",
    "    pyplot.show()\n",
    "    dataset[cols2].hist()\n",
    "    pyplot.show()\n",
    "    dataset[cols3].hist()\n",
    "    pyplot.show()    \n",
    "    # scatter plot matrix\n",
    "    scatter_matrix(dataset[cols1])\n",
    "    pyplot.show()\n",
    "    scatter_matrix(dataset[cols2])\n",
    "    pyplot.show()\n",
    "    scatter_matrix(dataset[cols3])\n",
    "    pyplot.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edc31c7e-e1ce-4a04-9424-41e88e09f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Pre-Processing\n",
    "def preProcessingData(dataset):\n",
    "\n",
    "    # 1. Data Cleaning\n",
    "      # There is no missing value. \n",
    "      # We could \"Outlier treatment\" but nothing was done here.  \n",
    "\n",
    "    # 2. Feature Selection\n",
    "    cols_X = ['sepal_length','sepal_width','petal_length','petal_width',\n",
    "              'ratio1','ratio2','ratio3','ratio4',\n",
    "              'ratio5','ratio6','ratio7','ratio8']\n",
    "    cols_Y = 'class'\n",
    "    seed = 7\n",
    "\n",
    "    # 3. Data Transform - Split out train : test datasets\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(dataset.loc[:, cols_X], \n",
    "                                                        dataset.loc[:, cols_Y], \n",
    "                                                        test_size=0.20,\n",
    "                                                        random_state = seed\n",
    "                                                        )\n",
    "    return train_X, test_X, train_Y, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5079417d-8652-4033-a745-9c56845fd0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applied Machine Learning Algorithm ... ... ...\n",
    "def evaluateAlgorithm(train_X, test_X, train_Y, test_Y):\n",
    "    \n",
    "    ##Machine Lreaning Algorithm, Parameter setting \n",
    "    model_LR = LogisticRegression()\n",
    "    \n",
    "    ##Cross Validation\n",
    "    cv_results = cross_val_score(model_LR, train_X, train_Y, cv = 4, \n",
    "                                 scoring='accuracy', n_jobs = -1, verbose = 0)\n",
    "    \n",
    "    print(\"\\nScores from cross validation: \", cv_results)\n",
    "    print(\"Mean accuracy score from Cross Validation\", cv_results.mean())\n",
    "    print(\"Std from Cross Validation\", cv_results.std())\n",
    "\n",
    "    ##Training & Fitting Algorithm with training Dataset\n",
    "    trained_Model = model_LR.fit(train_X, train_Y)\n",
    "\n",
    "    ##Evaluation of trained Algorithm (or Model) and result\n",
    "    pred_Class          = trained_Model.predict(test_X)\n",
    "    acc         = accuracy_score(test_Y, pred_Class)\n",
    "    classReport = classification_report(test_Y, pred_Class)\n",
    "    confMatrix  = confusion_matrix(test_Y, pred_Class) \n",
    "    print('\\nThe accuracy: {}'.format(acc))\n",
    "    print('The Classification Report:\\n {}'.format(classReport))\n",
    "    print('The Confusion Matrix:\\n {}'.format(confMatrix))\n",
    "    \n",
    "    #Save the trained Model\n",
    "    with open('trainedModel_LR.pickle', 'wb') as f:\n",
    "        pk.dump(trained_Model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0afc2772-f9a1-49a9-9c12-93c312ecf2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a (new or existing ) dataset to make prediction \n",
    "def loadPredictionDataset():\n",
    "    engine_str = (\n",
    "            'mysql+pymysql://{user}:{password}@{server}/{database}'.format(\n",
    "            user      =  'root',\n",
    "            password  =  'root888',\n",
    "            server    =  'localhost',\n",
    "            database  =  'datasciencerecipes'))\n",
    "    \n",
    "    engine = sa.create_engine(engine_str)\n",
    "    conn = engine.connect()\n",
    "    \n",
    "    #check whether connection is Successful or not\n",
    "    #if (conn): print(\"MySQL Connection is Successful ... ... ...\")    \n",
    "    #else:      print(\"MySQL Connection is not Successful ... ... ...\")\n",
    "\n",
    "    # MySQL Query - New Query is required for Prediction DataSet\n",
    "    query = '''\n",
    "    SELECT  sepal_length, \n",
    "            sepal_width, \n",
    "            petal_length, \n",
    "            petal_width, \n",
    "            round(sepal_length/sepal_width,2) as ratio1, \n",
    "            round(sepal_width/petal_length,2) as ratio2,\n",
    "            round(petal_length/petal_width,2) as ratio3,\n",
    "            round(petal_width/sepal_length,2) as ratio4,\n",
    "            round(sepal_width/sepal_length,2) as ratio5, \n",
    "            round(petal_length/sepal_width,2) as ratio6,\n",
    "            round(petal_width/petal_length,2) as ratio7,\n",
    "            round(sepal_length/petal_width,2) as ratio8\n",
    "    FROM irisdata;\n",
    "    '''\n",
    "    query_result = conn.execute(query)\n",
    "    dataset =  pd.DataFrame(query_result.fetchall(), \n",
    "                            columns =  query_result.keys())\n",
    "    conn.close()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f84335ec-6f78-4716-aa3c-61772c899b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the trained model and make prediction\n",
    "def loadTrainedModelForPrediction(pred_dataset):\n",
    "    f = open('trainedModel_LR.pickle', 'rb')\n",
    "    model = pk.load(f); f.close();\n",
    "    pred_Class = model.predict(pred_dataset)\n",
    "    pred_dataset.loc[:, 'classResult'] = pred_Class\n",
    "    return pred_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b252b97-2d93-46af-9a6f-c761a5882e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finalise the results and update the audiance\n",
    "def finaliseResult(result):\n",
    "\n",
    "    #Save Result in a CSV file\n",
    "    print(\"Save Result in a CSV file ... ... ...\")    \n",
    "    result.to_csv('finalResult.csv', index = False)\n",
    "    \n",
    "    #Save Result in a MySQl Table\n",
    "    engine_str = (\n",
    "            'mysql+pymysql://{user}:{password}@{server}/{database}'.format(\n",
    "            user      =  'root',\n",
    "            password  =  'root888',\n",
    "            server    =  'localhost',\n",
    "            database  =  'datasciencerecipes'))\n",
    "    \n",
    "    engine = sa.create_engine(engine_str)\n",
    "    conn = engine.connect()\n",
    "    \n",
    "    #check whether connection is Successful or not\n",
    "    #if (conn): print(\"MySQL Connection is Successful ... ... ...\")    \n",
    "    #else:      print(\"MySQL Connection is not Successful ... ... ...\")\n",
    "\n",
    "    print(\"Save Result in a MySQl Table ... ... ...\")    \n",
    "    result.to_sql(name='irisresult', con=engine, schema='datasciencerecipes', \n",
    "                   if_exists = 'replace', chunksize = 1000, index=False)\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcfad410-c1ca-46a9-be50-c1c095458097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris.data.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'iris.data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miris.data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 2. Load Dataset to which Machine Learning Algorithm to be applied\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mloadFrCSVFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m import2MySQL(dataset)\n\u001b[1;32m      8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m loadDataSetFrMySQLTable()\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mloadFrCSVFile\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(filename)\n\u001b[1;32m      4\u001b[0m col_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msepal_length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msepal_width\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpetal_length\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpetal_width\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcol_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'iris.data.csv'"
     ]
    }
   ],
   "source": [
    "# End-to-End Applied Machine Learning Recipes for Beginners and App-Developers\n",
    "if __name__ == '__main__':\n",
    "    filename = 'iris.data.csv'\n",
    "    \n",
    "    # 2. Load Dataset to which Machine Learning Algorithm to be applied\n",
    "    dataset = loadFrCSVFile(filename)\n",
    "    import2MySQL(dataset)\n",
    "    dataset = loadDataSetFrMySQLTable()\n",
    "    \n",
    "    # 3. Summarisation of Data to understand dataset (Descriptive Statistics)\n",
    "    summariseDataset(dataset)\n",
    "    \n",
    "    # 4. Visualisation of Data to understand dataset (Plots, Graphs etc.)\n",
    "    visualiseDataset(dataset)\n",
    "    \n",
    "    # 5. Data pre-processing and Data transformation (split into train-test datasets)\n",
    "    train_X, test_X, train_Y, test_Y = preProcessingData(dataset)\n",
    "    \n",
    "    # 6. Application of a Machine Learning Algorithm to training dataset \n",
    "    evaluateAlgorithm(train_X, test_X, train_Y, test_Y)\n",
    "    \n",
    "    # 7. Load the saved model and apply it to new dataset for prediction \n",
    "    pred_Dataset = loadPredictionDataset()\n",
    "    result = loadTrainedModelForPrediction(pred_Dataset)\n",
    "    finaliseResult(result)\n",
    "    \n",
    "    print('\\nEnd-to-End Applied Machine Learning Recipes for Developers\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a74b98-76c6-44cb-8b41-ca887b428281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d9a6f-4c50-445a-bc08-055f18544f18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
